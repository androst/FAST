#include "OpenVINOEngine.hpp"
#include <inference_engine.hpp>
#include <FAST/Utility.hpp>

namespace fast {

using namespace InferenceEngine;

void OpenVINOEngine::run() {
	try {
		// Copy input data
		reportInfo() << "OpenVINO: Processing input nodes.." << reportEnd();
		int batchSize = -1;
		for(const auto& node : mInputNodes) {
			auto tensor = node.second.data;
			batchSize = tensor->getShape()[0];
			auto access = tensor->getAccess(ACCESS_READ);
			float* tensorData = access->getRawData();
			Blob::Ptr input = m_inferRequest->GetBlob(node.first);

			// Dynamic batch size
			if(m_maxBatchSize > 1)
                m_inferRequest->SetBatch(batchSize);

			auto input_data = input->buffer().as<PrecisionTrait<Precision::FP32>::value_type * >();
			std::memcpy(input_data, tensorData, input->byteSize());
		}
		reportInfo() << "OpenVINO: Finished processing input nodes." << reportEnd();

		// Execute network
        m_inferRequest->Infer();
		reportInfo() << "OpenVINO: Network executed." << reportEnd();

		// Copy output data
		for (auto& node : mOutputNodes) {
			Blob::Ptr output = m_inferRequest->GetBlob(node.first);
			auto outputData = (output->buffer().as<::InferenceEngine::PrecisionTrait<Precision::FP32>::value_type *>());
			auto copied_data = make_uninitialized_unique<float[]>(output->byteSize());
			std::memcpy(copied_data.get(), outputData, output->byteSize());
			auto tensor = Tensor::New();
			tensor->create(std::move(copied_data), node.second.shape);
			node.second.data = tensor;
		}
		reportInfo() << "OpenVINO: Finished processing output nodes." << reportEnd();
	}
	catch (::InferenceEngine::details::InferenceEngineException &e) {
		throw Exception("Inference error occured during OpenVINO::run: " + std::string(e.what()));
	}
}

void OpenVINOEngine::loadPlugin(std::string deviceName) {

    reportInfo() << "OpenVINO: Inference plugin setup complete for device type " << deviceName << reportEnd();

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    auto input_model = getFilename();
    CNNNetwork network;
    if(input_model.empty()) { // If filename is not set, load from memory instead
        // Read from memory
        std::string strModel(m_model.begin(), m_model.end());
        network = m_inferenceCore->ReadNetwork(strModel, make_shared_blob<uint8_t>({Precision::U8, {m_weights.size()}, C}, m_weights.data()));
    } else {
        // Read from file
        if (!fileExists(input_model))
            throw FileNotFoundException(input_model);
        network = m_inferenceCore->ReadNetwork(fileNameToString(input_model));
    }

    //network.setBatchSize(1);
    reportInfo() << "OpenVINO: Network loaded." << reportEnd();

    // --------------------------- Prepare input blobs -----------------------------------------------------
    int counter = 0;
    for(auto& input : network.getInputsInfo()) {
        auto input_info = input.second;
        auto input_name = input.first;
        input_info->setPrecision(Precision::FP32);
        // TODO shape is reverse direction here for some reason..
        TensorShape shape;
        auto data = input_info->getInputData();
        auto dims = data->getDims();
        for(int i = 0; i < dims.size(); ++i) { // TODO why reverse??
            shape.addDimension(dims[i]);
        }

        if(input_info->getLayout() == Layout::NCHW || input_info->getLayout() == Layout::NCDHW) {
            addInputNode(counter, input_name, NodeType::IMAGE, shape);
        } else {
            addInputNode(counter, input_name, NodeType::TENSOR, shape);
        }
        reportInfo() << "Found input node: " << input_name << " with shape " << shape.toString() << reportEnd();
        counter++;
    }

    // --------------------------- Prepare output blobs ----------------------------------------------------
    counter = 0;
    for(auto& output : network.getOutputsInfo()) {
        auto info = output.second;
        auto name = output.first;
        info->setPrecision(Precision::FP32);
        TensorShape shape;
        for(auto dim : info->getDims())
            shape.addDimension(dim);
        addOutputNode(counter, name, NodeType::TENSOR, shape);
        reportInfo() << "Found output node: " << name << " with shape " << shape.toString() << reportEnd();
        counter++;
    }
    reportInfo() << "OpenVINO: Node setup complete." << reportEnd();

    std::map<std::string, std::string> config;
    if(m_maxBatchSize > 1) {
        config[PluginConfigParams::KEY_DYN_BATCH_ENABLED] = PluginConfigParams::YES;
        network.setBatchSize(m_maxBatchSize);
    }

    ExecutableNetwork executable_network = m_inferenceCore->LoadNetwork(network, deviceName, config);

    m_inferRequest = executable_network.CreateInferRequestPtr();
    setIsLoaded(true);
    reportInfo() << "OpenVINO: Network fully loaded." << reportEnd();
}

void OpenVINOEngine::load() {
    m_inferenceCore = std::make_shared<Core>();
    auto devices = m_inferenceCore->GetAvailableDevices();
    reportInfo() << "Available OpenVINO devices:" << reportEnd();
    for(auto&& device : devices)
        reportInfo() << device << reportEnd();
    if(m_deviceType == InferenceDeviceType::ANY) {
        try {
            loadPlugin("GPU");
        } catch(::InferenceEngine::details::InferenceEngineException &e) {
            try {
                reportInfo() << "Failed to get GPU plugin for OpenVINO inference engine: " << e.what() << reportEnd();
                reportInfo() << "Trying VPU/Myriad/Neural compute stick plugin instead.." << reportEnd();
                loadPlugin("MYRIAD");
            } catch(::InferenceEngine::details::InferenceEngineException &e) {
                try {
                    reportInfo() << "Failed to get GPU/VPU plugin for OpenVINO inference engine: " << e.what()
                                 << reportEnd();
                    reportInfo() << "Trying CPU plugin instead.." << reportEnd();
                    loadPlugin("CPU");
                } catch(::InferenceEngine::details::InferenceEngineException &e) {
                    reportError() << e.what() << reportEnd();
                    throw Exception("Failed to load any device in OpenVINO IE");
                }
            }
        }
    } else {
        std::map<InferenceDeviceType, std::string> deviceMapping = {
                {InferenceDeviceType::GPU, "GPU"},
                {InferenceDeviceType::CPU, "CPU"},
                {InferenceDeviceType::VPU, "MYRIAD"},
        };

        try {
            loadPlugin(deviceMapping[m_deviceType]);
        } catch(::InferenceEngine::details::InferenceEngineException &e) {
            throw Exception(std::string("Failed to load device ") + deviceMapping[m_deviceType] + " in OpenVINO inference engine");
        }
    }
}

ImageOrdering OpenVINOEngine::getPreferredImageOrdering() const {
    return ImageOrdering::ChannelFirst;
}

std::string OpenVINOEngine::getName() const {
    return "OpenVINO";
}

std::string OpenVINOEngine::getDefaultFileExtension() const {
    return "xml";
}

OpenVINOEngine::~OpenVINOEngine() {
    //if(m_inferState != nullptr)
    //    delete m_inferState;
}

}
