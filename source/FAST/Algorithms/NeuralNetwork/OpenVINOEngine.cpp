#include "OpenVINOEngine.hpp"
#include <inference_engine.hpp>
#include <FAST/Utility.hpp>

namespace fast {

using namespace InferenceEngine;

void OpenVINOEngine::run() {
	try {
		// Copy input data
		reportInfo() << "OpenVINO: Processing input nodes.." << reportEnd();
		for (const auto& node : mInputNodes) {
			auto tensor = node.second.data;
			auto access = tensor->getAccess(ACCESS_READ);
			float* tensorData;
			switch (tensor->getShape().getDimensions()) {
			case 2:
				tensorData = access->getData<2>().data();
				break;
			case 3:
				tensorData = access->getData<3>().data();
				break;
			case 4:
				tensorData = access->getData<4>().data();
				break;
			case 5:
				tensorData = access->getData<5>().data();
				break;
			case 6:
				tensorData = access->getData<6>().data();
				break;
			default:
				throw Exception("Invalid tensor dimension size");
			}
			Blob::Ptr input = m_inferRequest->GetBlob(node.first);
			auto input_data = input->buffer().as<PrecisionTrait<Precision::FP32>::value_type * >();
			std::memcpy(input_data, tensorData, input->byteSize());
		}
		reportInfo() << "OpenVINO: Finished processing input nodes." << reportEnd();

		// Execute network
		m_inferRequest->Infer();
		reportInfo() << "OpenVINO: Network executed." << reportEnd();

		// Copy output data
		for (auto& node : mOutputNodes) {
			Blob::Ptr output = m_inferRequest->GetBlob(node.first);
			auto outputData = (output->buffer().as<::InferenceEngine::PrecisionTrait<Precision::FP32>::value_type *>());
			auto copied_data = make_uninitialized_unique<float[]>(output->byteSize());
			std::memcpy(copied_data.get(), outputData, output->byteSize());
			auto tensor = Tensor::New();
			tensor->create(std::move(copied_data), node.second.shape);
			node.second.data = tensor;
		}
		reportInfo() << "OpenVINO: Finished processing output nodes." << reportEnd();
	}
	catch (::InferenceEngine::details::InferenceEngineException &e) {
		throw Exception("Inference error occured during OpenVINO::run: " + std::string(e.what()));
	}
}
void OpenVINOEngine::loadPlugin(std::string deviceType) {
    PluginDispatcher dispatcher({""});
    m_inferencePlugin = std::make_shared<::InferenceEngine::InferencePlugin>(dispatcher.getPluginByDevice(deviceType));
    reportInfo() << "OpenVINO: Inference plugin setup complete for device type " << deviceType << reportEnd();

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    auto input_model = getFilename();
    if(!fileExists(input_model))
        throw FileNotFoundException(input_model);

    CNNNetReader network_reader;
    network_reader.ReadNetwork(fileNameToString(input_model));
    network_reader.ReadWeights(fileNameToString(input_model).substr(0, input_model.size() - 4) + ".bin");
    CNNNetwork network = network_reader.getNetwork();
    //network.setBatchSize(1);
    reportInfo() << "OpenVINO: Network loaded." << reportEnd();

    // --------------------------- Prepare input blobs -----------------------------------------------------
    int counter = 0;
    for(auto& input : network.getInputsInfo()) {
        auto input_info = input.second;
        auto input_name = input.first;
        input_info->setPrecision(Precision::FP32);
        // TODO shape is reverse direction here for some reason..
        TensorShape shape;
        auto dims = input_info->getDims();
        for(int i = dims.size() - 1; i >= 0; --i) // TODO why reverse??
            shape.addDimension(dims[i]);

        if(shape.getDimensions() > 3) {
            input_info->setLayout(Layout::NCHW);
            addInputNode(counter, input_name, NodeType::IMAGE, shape);
        } else {
            addInputNode(counter, input_name, NodeType::TENSOR, shape);
        }
        reportInfo() << "Found input node: " << input_name << " with shape " << shape.toString() << reportEnd();
        counter++;
    }

    // --------------------------- Prepare output blobs ----------------------------------------------------
    counter = 0;
    for(auto& output : network.getOutputsInfo()) {
        auto info = output.second;
        auto name = output.first;
        info->setPrecision(Precision::FP32);
        TensorShape shape;
        for(auto dim : info->getDims())
            shape.addDimension(dim);
        addOutputNode(counter, name, NodeType::TENSOR, shape);
        reportInfo() << "Found output node: " << name << " with shape " << shape.toString() << reportEnd();
        counter++;
    }
    reportInfo() << "OpenVINO: Node setup complete." << reportEnd();

    ExecutableNetwork executable_network = m_inferencePlugin->LoadNetwork(network, {});

    m_inferRequest = executable_network.CreateInferRequestPtr();
    setIsLoaded(true);
    reportInfo() << "OpenVINO: Network fully loaded." << reportEnd();
}

void OpenVINOEngine::load() {
    try {
        loadPlugin(getDeviceName(TargetDevice::eGPU));
    } catch(::InferenceEngine::details::InferenceEngineException &e) {
        reportInfo() << "Failed to get GPU plugin for OpenVINO inference engine: " << e.what() << reportEnd();
        reportInfo() << "Selecting CPU plugin instead.." << reportEnd();
        loadPlugin(getDeviceName(TargetDevice::eCPU));
    }

}

ImageOrdering OpenVINOEngine::getPreferredImageOrdering() const {
    return ImageOrdering::CHW;
}

std::string OpenVINOEngine::getName() const {
    return "OpenVINO";
}

std::string OpenVINOEngine::getDefaultFileExtension() const {
    return "xml";
}
}
